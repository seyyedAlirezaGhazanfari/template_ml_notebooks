{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5ef68b",
   "metadata": {},
   "source": [
    "<div align=center dir=rtl>\n",
    "<font face=\"vazir\" size=5>\n",
    "به نام خدا\n",
    "</font>\n",
    "<br> <br>\n",
    "<font face=\"vazir\"size=3>\n",
    "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "<br> <br>\n",
    "    \n",
    "<font face=\"vazir\" color=red size=5>\n",
    "مقدمه‌ای بر یادگیری ماشین\n",
    "</font>\n",
    "\n",
    "<hr/> <br>\n",
    "    \n",
    "<font face=\"vazir\" color=#228B22 size=8>\n",
    "<b>فصل سوم: یادگیری، ارزیابی و تنظیم‌کردن مدل‌ها \n",
    "</font>\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "<font face=\"vazir\" size=2>\n",
    "علیرضا گرگوری مطلق، پیمان ناصری، علیرضا حیدری\n",
    "</font>\n",
    "<hr>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2644e",
   "metadata": {},
   "source": [
    "<font face=\"vazir\">\n",
    "<div dir=rtl align=right>\n",
    "<div id=\"table-of-contents\">\n",
    "  <h1 align=center><font face=\"vazir\" color=#00008B >فهرست مطالب</font></h1>\n",
    "    \n",
    "  <ul>\n",
    "    <li><a href=\"#intro\">مقدمه</a>\n",
    "    <li><a href=\"#basics\">مفاهیم اولیه</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#hyperplane\">ابر صفحه</a></li>\n",
    "        <li><a href=\"#margin\">حاشیه</a></li>\n",
    "  </ul>\n",
    "        <li><a href=\"#maximalMarginClassifier\">طبقه‌بندی بیشینه‌نمای حاشیه</a></li>\n",
    "   <ul>\n",
    "        <li><a href=\"#GradientDescent\">نزول گرادیان</a></li>\n",
    "        <li><a href=\"#GD-MMC\">نزول گرادیان برای طبقه بندی بیشینه نمای حاشیه </a></li>\n",
    "  </ul>\n",
    "       <li><a href=\"#svm\">طبقه‌بندی بردار پشتیبانی</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#limit1\">محدودیت اول: وابستگی به نقاط مرزی </a></li>\n",
    "          <ul>\n",
    "              <li><a href=\"#solution1\">راه حل اول: طبقه بندی نرم حاشیه</a></li>\n",
    "          </ul>\n",
    "  </ul>    \n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee51e14-31fa-4b7d-9cd4-7fb83c5029ac",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"intro\">\n",
    "    <h1>\n",
    "        <font face=\"vazir\" color=#0000CD size=7>مقدمه</font>\n",
    "    </h1>\n",
    "    <hr>\n",
    "    <div dir=rtl>\n",
    "        <font face=\"vazir\" size=4>\n",
    "            در این فصل قصد داریم که به معرفی مفاهیم پایه‌ای و البته مهمی در یادگیری ماشین بپردازیم. تمرکز ما برای معرفی مفاهیم مرتبط،\n",
    "            مدل <b>ماشین‌های بردار پشتیبان</b>\n",
    "            (Support Vector Machines)\n",
    "می باشد؛ در ابتدا به پیاده‌سازی مدل ساده‌شده‌ای از SVM پرداخته و سپس به معرفی کتابخانه\n",
    "Scikit-Learn\n",
    "که یکی از مهمترین و قدرتمندترین کتابخانه‌های موجود در زمینه یادگیری ماشین است میپردازیم.\n",
    "در ادامه با استفاده از ماژول‌های این کتابخانه مدل‌های پیچیده‌تری را پیاده‌سازی خواهیم کرد و نحوه ارزیابی و انتخاب مدل‌های مناسب را بر اساس معیارهای مدنظرمان فرا خواهیم گرفت.\n",
    "<br> <br>\n",
    "پیش از شروع بحث بهتر است به معرفی ابرصفحه (Hyperplane) و نیز حاشیه (Margin) بپردازیم.\n",
    "<br><b> (در سراسر این فصل فرض می‌شود که $N$ تعداد نمونه‌های ما و $p$ تعداد ویژگی‌های هر نمونه می‌باشد.) \n",
    "        </font>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7524a-0055-4347-bb45-b5c37cc819c4",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"basics\">\n",
    "    <h1>\n",
    "        <font face=\"vazir\" color=#0000CD size=7>مفاهیم اولیه</font>\n",
    "    </h1>\n",
    "    <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704435b-b4c1-462d-a07b-122df35f3ad5",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"hyperplane\">\n",
    "    <h2>\n",
    "        <font face=\"vazir\" color=#FF8C00 size=6>ابرصفحه</font>\n",
    "    </h2>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "یک ابرصفحه در فضای *p*بعدی در واقع زیرفضایی affine از بعد *p-1* می باشد.\n",
    "معادله زیر بیان کننده یک ابرصفحه در فضای *p*بعدی میباشد:  \n",
    "<br><div align=center> $f(X) = b + w_1X_1 + w_2X_2 + ... + w_pX_p = w^T X + b = 0 $</div>\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br>در معادله بالا بردار $w = (w_1, w_2, ..., w_p)^T $ بردار نرمال بر ابرصفحه مذکور است و در واقع نشانگر جهتی میباشد که بر ابرصفحه عمود است. به این بردار، بردار وزن نیز می‌گویند که در ادامه دلیل آن مشخص خواهد شد.\n",
    "    همچنین b نیز عرض‌ازمبدا(intercept) ابرصفحه می‌باشد که از آن با نام بایاس نیز یاد می‌شود.\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<div align=center>\n",
    "<img src=\"resources/hyperplane.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br> همانطور که مشاهده میکنیم، در فضای 2 بعدی ابرصفحه خط راستی با شیب $w_1$ و عرض از مبدا $b$ می باشد و در فضای 3 بعدی نیز، یک ابرصفحه در واقع همان صفحه است. در ابعاد بالاتر نیز با تعمیم این صفحه، دارای ابرصفحه‌ای خواهیم بود که بردار $w$ بر آن عمود می باشد و فضای\n",
    "*$p$*-بعدی را به 2 نیم فضا تقسیم میکند؛ به گونه‌ای که در یک سمت این ابرصفحه، \n",
    "$f(X) > 0$ است و در سمت دیگر آن $f(X) < 0$ خواهد بود.\n",
    "</div></font>\n",
    "\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br> حال مسئله‌ای را در نظر بگیرید که میخواهیم داده‌های دو کلاس را از یکدیگر تفکیک کنیم. در صورتی که بتوانیم ابرصفحه مناسبی را پیدا کنیم که داده‌های هر کلاس در یک طرف این ابرصفحه قرار بگیرند، و برچسب کلاس‌های هر گروه را به صورت $Y^{(i)} \\in \\{\\pm 1\\}$ کدگذاری کنیم، برای هر نمونه معادله زیر برقرار خواهد بود:\n",
    "<br> <div align=center> $ Y^{(i)}f(X^{(i)}) > 0 \\quad \\forall i \\in \\{1,2,...,N\\} $</div>\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br>در شکل زیر نمونه‌ای از یک صفحه جداکننده دو کلاس در فضای 2بعدی نمایش داده شده است:\n",
    "</div></font>\n",
    "    \n",
    "<div align=center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplane.png\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c34675-b821-49a9-99a9-5ecdafb8738a",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"margin\">\n",
    "    <h2>\n",
    "        <font face=\"vazir\" color=#FF8C00 size=6>حاشیه (Margin)</font>\n",
    "    </h2>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    " یک راه‌حل معقول برای جداسازی داده‌های دو کلاس، پیدا کردن ابرصفحه‌ای است که بیشترین حاشیه (Margin) را نسبت به داده‌های دو کلاس داشته باشد؛\n",
    "    منظور از حاشیه فاصله این ابرصفحه از نزدیک‌ترین داده هر کلاس است:\n",
    "</div></font>\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplane_margin.png\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9e7c0-a059-4dd9-8d7c-b23c5865d575",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"maximalMarginClassifier\">\n",
    "    <h1>\n",
    "        <font face=\"vazir\" color=#0000CD size=7>طبقه‌بند بیشینه‌نمای حاشیه (Maximal Margin Classifier)</font>\n",
    "    </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div dir=rtl><font face=\"vazir\" size=4>\n",
    "از آنجا که هر ابرصفحه جداکننده‌ای را می‌توان با انتقال یا چرخش کوچکی به ابرصفحه دیگری تبدیل نمود که همچنان تمایزدهنده دو کلاس است، بی‌نهایت ابرصفحه میتواند جواب مسئله ما باشد؛ پس باید بتوانیم ابرصفحه‌ای که به طور متقارن از هر دو کلاس بیشینه فاصله را دارد، پیدا نماییم؛ به این ابرصفحه، <b>ابرصفحه جداکننده بهینه</b> \n",
    "(Optimal Separating Hyperplane)\n",
    "    یا <b>ابرصفحه جداکننده بیشینه</b>\n",
    "(Maximal Separating Hyperplane)\n",
    "میگوییم.\n",
    "</font></div>\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplanes.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br> <br>\n",
    "بنابراین مسئله ما پیدا کردن ابرصفحه‌ای شده است که حداکثر حاشیه ممکن از دو کلاس را داشته باشد؛ می‌توان به این مسئله به چشم پیدا کردن دو ابرصفحه دیگر \n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "که فاصله برابر و بیشینه‌ای از ابرصفحه مطلوب ما\n",
    "$\\mathcal{H}_0$\n",
    "دارند نیز نگاه کرد؛ به طوری که داده‌های دو کلاس در دو سمت متفاوت \n",
    "$\\mathcal{H}_0$\n",
    "قرار بگیرند و هیچ نمونه‌ای نیز در فضای بین آن‌ها نیفتد.\n",
    "شکل زیر بیانگر مطالب بالا می‌باشد:\n",
    "</div></font>\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "<img src=\"resources/extermum_hyperplanes.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<font face=\"vazir\" size=4>\n",
    "<div dir=rtl>\n",
    "<br>بدون از دست دادن کلیت مسئله، می‌توان دو صفحه \n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "را به صورت زیر تعریف نمود:  \n",
    "<br><div align=center>  $\\mathcal{H}_1: \\quad w^T x^{(i)} + b \\geq 1 \\qquad if \\quad y^{(i)}=1 $</div>\n",
    "    <br><div align=center>  $\\mathcal{H}_2: \\quad w^T x^{(i)} + b \\leq -1 \\qquad if \\quad y^{(i)}=-1 $</div>\n",
    "    </div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br>دو معادله بالا را می‌توان با یکدگیر ترکیب نمود و به صورت زیر نوشت:\n",
    "<br><div align=center> $y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\quad \\forall i \\in \\{1,2,...,N\\} $</div>\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br>حال باید فاصله بین دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "را پیدا کنیم تا بتوانیم با بیشینه کردن آن، صفحه\n",
    "$\\mathcal{H}_0$\n",
    "را به صورت ابرصفحه‌ای موازی با آنها که این فاصله را به دو قسمت برابر تقسیم میکند، مشخص نماییم.\n",
    "<br> <br>\n",
    "یک راه توصیف حاشیه می‌تواند اینگونه باشد که حاشیه را برداری عمود بر دو ابرصفحه مذکور در نظر گرفت که اندازه این بردار برابر با مقدار حاشیه خواهد بود. پس با تعریف بردار مناسبی برای توصیف این فاصله، کافی است اندازه آن را پیدا کنیم.\n",
    "<br>از آنجا که بردار وزن‌های\n",
    "$w$\n",
    " عمود بر ابرصفحه موردنظر ما است، می توان بردار یکه $u$ را برداری هم‌جهت با بردار $w$ در نظر گرفت؛\n",
    "پس بردار $u$ به صورت زیر می‌باشد:\n",
    "<br> <div align=center> <font size=4> $ u = \\frac{w}{{\\lVert w \\rVert}_2} $</font></div>\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "حال کافی است بردار $u$ را در مقدار اسکالر $m$ ضرب نمود تا \n",
    "بردار $k$ را پیدا کرد که اندازه آن برابر با حاشیه $m$ می‌باشد و در راستای موردنظر نیز خواهد بود:\n",
    "<br> <div align=center> <font size=5> $ k = m.u = m.\\frac{w}{{\\lVert w \\rVert}_2} $</font></div>\n",
    "</div></font>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "شکل زیر توضیحات بالا را به خوبی ترسیم می‌نماید:\n",
    "<div align=center>\n",
    "<img src=\"resources/scalers.png\" width=\"400\">\n",
    "</div>\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "حال نقطه‌ی دلخواه $x_0$\n",
    "را بر روی ابرصفحه\n",
    "$\\mathcal{H}_2$   \n",
    "در نظر بگیرید و آن را با بردار $k$ جمع کنید تا به نقطه $z_0$ بر روی ابرصفحه \n",
    "$\\mathcal{H}_1$ برسید.\n",
    "<div align=center>\n",
    "<img src=\"resources/find_k.png\" width=\"400\">\n",
    "</div>\n",
    "</div></font>\n",
    "  \n",
    "<font face=\"vazir\" size=4>\n",
    "<div dir=rtl>\n",
    "از آنجا که نقطه $x_0$ بر روی $\\mathcal{H}_2$ و $z_0$ بر روی $\\mathcal{H}_1$ قرار گرفته‌اند، پس در معادلات زیر صدق می‌کنند:\n",
    "<br><br> <div align=center> $w^T z_0 + b = 1 $</div>\n",
    "<br> <div align=center> $w^T x_0 + b = -1 $</div>\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "با کم کردن دو معادله بالا از یکدیگر خواهیم داشت:\n",
    "<br> <div align=center> $w^T (z_0 - x_0) = 2$</div>\n",
    "</div></font>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "از طرفی $z_0 = x_0 + k$ می‌باشد؛ پس\n",
    "<br><br> <div align=center> $w^T k = w^T . m\\frac{w}{{\\lVert w \\rVert}_2} = 2 \\Rightarrow $</div>\n",
    "<br><br> <div align=center> $ m.\\frac{{\\lVert w \\rVert}_2 ^2}{{\\lVert w \\rVert}_2} = m.{\\lVert w \\rVert}_2 = 2 \\Rightarrow$</div>\n",
    "<br><br> <div align=center> <font size=4> $ m = \\frac{2}{{\\lVert w \\rVert}_2} $ </font></div><br>\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "پس با توجه به نتایج بالا، فاصله بین دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "برابر با\n",
    "$ m = \\frac{2}{{\\lVert w \\rVert}_2} $\n",
    "می‌باشد و جهت بیشینه کردن این مقدار، باید مخرج آن، یعنی اندازه بردار نرمال را کمینه نمود.\n",
    "به نمونه‌هایی که بر روی دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "قرار میگیرند و در تعیین نمودن ابرصفحه جداکننده بهینه نقش دارند، بردارهای پشتیبان (support vectors) گفته می‌شود. شکل زیر به طور نمادین تمام نتایج بالا را دارا میباشد:\n",
    "</div></font>\n",
    "    \n",
    "<div> <br>\n",
    "<div align=center>\n",
    "<img src=\"resources/support_vectors.png\" width=\"400\">\n",
    "</div>\n",
    "</div>\n",
    "    \n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "<br> با جمع بندی نتایج و توضیحات بالا، ما به‌دنبال حل کردن مسئله بهینه‎سازی زیر می‌باشیم:\n",
    "<div align=center>\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b} \\quad & \\frac{1}{2}||w||_2 ^2\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\quad \\forall i \\in \\{1,2,...,N\\}\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "</div></div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><br><div dir=rtl> (همانطور که می‌دانید کمینه کردن اندازه یک بردار با کمینه کردن توان دو اندازه آن بردار، معادل می‌باشد و برای سادگی محاسبات در ادامه از این نکته استفاده کرده‌ایم!) <br>\n",
    "(همچنین استفاده از ضریب $\\frac{1}{2}$ نیز جهت تسهیل نتایج مشتق گرفتن در ادامه می‌باشد!)\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br> مسئله مقید بالا یک مسئله بهینه‌سازی محدب می‌باشد و در ادبیات بهینه‌سازی محدب از آن به عنوان برنامه‌نویسی مرتبه دوم \n",
    "(Quadratic Programming)\n",
    "یاد می‌شود. در صورت علاقمندی به مطالعه بیشتر راجع‌به Quadratic Programming می‌توانید به <a href=\"https://en.wikipedia.org/wiki/Quadratic_programming\">لینک</a> زیر مراجعه نمایید:\n",
    "</div></font>\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "با توجه به محدب بودن مسئله بالا، وجود جوابی جهت کمینه کردن تابع هدف تضمین می‌شود؛ پس می‌توان با معرفی تابع هزینه مناسبی برای مسئله بالا، به طوری که عبارت را از حالت مقید خارج سازد، آن را با روش‌های مرسوم بهینه‌سازی همچون Gradient Descent .حل نماییم\n",
    "</div> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd42a52-cab6-400d-b9a8-8239c7844c62",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"GradientDescent\">\n",
    "    <h2>\n",
    "        <font face=\"vazir\" color=#FF8C00 size=6>معرفی Gradient Descent</font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "</div>\n",
    "<font face=\"vazir\" size=4><div dir=rtl>    \n",
    "در این بخش به طور خلاصه به روش Gradient Descent\n",
    "که الگوریتمی از خانواده الگوریتم‌های مرتبه اول در بهینه سازی است و جهت پیداکردن کمینه محلی (Local Minimum)\n",
    "    و در توابع محدب کمینه جهانی (Global Minimum)\n",
    "به کار می‌رود، اشاره می‌کنیم. در فصل‌های بعد به طور کامل‌تر با این روش آشنا خواهید شد. <br>\n",
    "این روش ساده از مرسوم‌ترین و پرطرفدارترین روش‌های بهینه‌سازی در یادگیری ماشین و خصوصا یادگیری عمیق می‌باشد.\n",
    "در واقع با استفاده از این روش ما سعی داریم که تابع هدف(یا در ادبیات یادگیری ماشین، تابع هزینه) خود را کمینه کنیم و پارامترهای بهینه را جهت این هدف پیدا کنیم. <br>\n",
    "همانطور که می‌دانیم، مشتق تابع در یک نقطه، نمایانگر شیب تابع در آن نقطه و جهتی است که تابع در راستای آن بیشترین افزایش مقدار را دارد؛ بنابراین با حرکت کردن در خلاف جهت مشتق تابع می‌توانیم کمینه مورد نظر را پیدا کنیم. این روش ساده و تعمیم‌های آن مبنای بهینه‌سازی بسیاری از مدل‌های یادگیری ماشین خواهند بود که در ادامه درس خواهید دید.\n",
    "</div></font>\n",
    "\n",
    "<br>\n",
    "<div align=center>\n",
    "<img src=\"resources/ball.png\" width=\"500\">\n",
    "</div>\n",
    "    \n",
    "   \n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br> \n",
    "فرض کنید هدف ما پیداکردن پارامترهایی است که تابع هزینه \n",
    "$J(w)$ را کمینه میکنند؛ یعنی:\n",
    "    \n",
    "<div align=center><font face=\"vazir\" size=4> $w^{opt} = \\underset{w}{\\mathrm{argmin}}\\hspace{1mm} J(w)$ </font></div></div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "بنابراین روش ما برای پیداکردن پارامترهای بهینه به صورت زیر خواهد بود: <br>\n",
    "<br> 1. پارامترهای موردنظر را مقداردهی اولیه نماییم.(مقداردهی اولیه معمولا به صورت تصادفی خواهد بود، هرچند روش‌های خیلی پیشرفته‌تر و بهتری جهت مقداردهی اولیه وجود دارد.)\n",
    "<br><br> 2. مقدار و جهت آپدیت پارامترها را به کمک مشتق تابع به ازای آن پارامترها پیدا کنیم:\n",
    "<br><br> <font size=\"4\"> <div align=center> $\\Delta w = -\\eta \\frac{\\partial J(w)}{\\partial w}$ </div></font></div></font>\n",
    "<br>\n",
    "<font face=\"vazir\" size=4><div dir=rtl> همانطور که از توضیحات بالا مشخص است، این روش یک روش تکرارشونده است و مقدار حرکت در هر گام را با استفاده از پارامتر $\\eta$ که از آن به نام ضریب یادگیری (Learning Rate) یاد می‌شود تنظیم میکنیم. \n",
    "<br><br> 3. پارامترهای موردنظر را به‌روز نماییم:\n",
    "<br><br> <font size=\"4\"> <div align=center> $w^{new} = w^{old} + \\Delta w$ </div></font></div></font>\n",
    "<font face=\"vazir\" size=4><div dir=rtl> <br>\n",
    "4. مراحل 2و3 را آنقدر تکرار میکنیم تا جایی‌که مشتق تابع هزینه در آن نقطه صفر شود و در نتیجه $w^{new} = w^{old}$. پارامترهای بهینه برابر با این مقدار $w$ می‌باشد. \n",
    "</div></font>\n",
    "    \n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl><br><br> از موارد بسیار مهم در این روش تنظیم مقدار ضریب یادگیری $\\eta$  می‌باشد؛\n",
    "همانطور که گفته شد، انداره حرکت ما در هر گام به سمت کمینه محلی را این ضریب کنترل می‌کند.\n",
    "در صورتی که مقدار این ضریب کوچک باشد، قدم‌های ما بسیار آرام خواهند بود و در نتیجه الگوریتم ما زمان بیشتری جهت همگراشدن نیاز دارد.\n",
    "همچنین در صورتی که مقدار این ضریب بسیار بزرگ باشد، الگوریتم ما در نزدیکی نقطه بهینه ممکن است دچار پرش به عقب و جلو شود و هیچوقت به مقدار بهینه نرسد؛ شکل زیر به خوبی این اثر را نمایش می‌دهد:\n",
    "</div></font>\n",
    "\n",
    "<br>\n",
    "<div align=center>\n",
    "<img src=\"resources/sgd_learning_rates.gif\" width=\"800\">\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf23f9-248b-49df-a2d4-26d74e1ce317",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"GD-MMC\">\n",
    "    <h2>\n",
    "        <font family=\"vazir\" color=#FF8C00 size=6>Gradient Descent برای Maximal Margin Classifier</font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "</div>\n",
    "<div dir=rtl><font face=\"vazir\" size=4> \n",
    "از آنجایی که اشاره شد مسئله بهینه‌سازی Maximal Margin Classifier یک مسئله محدب است،\n",
    "پس با تعریف تابع هزینه مناسبی برای آن و خارج کردن مسئله از حالت مقید، می‌توانیم با استفاده از روش Graident Descent، بردار $\\beta$ بهینه که باعث بدست‌آمدن ابرصفحه مناسب ما است را پیدا کنیم. <br>\n",
    "<br> تابع هزینه ما برای این مسئله به صورت زیر تعریف می‌شود: </font></div><br>\n",
    "\n",
    "<div align=center><font size=4> $J(w, b) = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 + C \\sum_{i=1}^{N} \\max\\big(0, 1-y^{(i)}(w^T x^{(i)} + b) \\big)$ </font></div>\n",
    "\n",
    "    \n",
    "<div dir=rtl><font face=\"vazir\" size=4> <br>\n",
    "تا بدین‌جا بخشی از تابع هزینه بالا باید برای شما آشنا بنظر برسد؛ جمله اول تابع هزینه بالا درواقع نقش بیشینه‌کردن حاشیه را برای ما دارد که هدف ما نیز بوده است.\n",
    "جمله دوم اما، نقش قیدهای مسئله بهینه‌سازی ما را ایفا می‌کنند که تضمین‌گر برچسب‌زنی درست نمونه‌ها و جداشدن داده‌های دو کلاس توسط یک ابرصفحه مطلوب با مقدار حاشیه موردنظر است. به جمله دوم در عبارت بالا Hinge Loss نیز گفته می‌شود.\n",
    "<br> در واقع Hinge Loss  با جمع بستن بر روی تمام نمونه‌هایی که حداقل حاشیه موردنظر ما را رعایت نکرده‌اند، مقدار خطایی را به تابع هزینه ما اضافه میکند و در صورتی که نمونه‌ای به درستی برچسب زده شده باشد مقدار این خطا برابر با 0 خواهد بود. \n",
    "این خطا با استفاده از هایپرپارامتر $C$ نیز کنترل می‌شود که در بخش‌های بعد اثر آن را خواهید دید.\n",
    "پس توانستیم با استفاده از Hinge Loss مسئله را بهینه‌سازی خود را از حالت مقید خارج کنیم.\n",
    "<br><br> خبر خوب این است که تابع هزینه بالا جمع دو تابع محدب است و مجموع تعدادی تابع محدب، تابعی محدب خواهد بود؛ در نتیجه می‌توانیم با استفاده از روش Gradient Descent بردار $w$ و $b$ بهینه را پیدا کنیم که با $w^*, b^*$ آن‌ها را نمایش می‌دهیم:\n",
    "    </font></div>\n",
    "    <br>\n",
    "<div align=center><font face=\"vazir\" size=4>$w^*, b^* = \\underset{w, b}{\\mathrm{argmin}}\\hspace{1mm} J(w, b)$</font></div>    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "برای پیداکردن گرادیان تابع هزینه باید بین دو حالت تمایز قائل شویم: \n",
    "<br><br>\n",
    "<div align=center>\n",
    "\\begin{cases}\n",
    "    J_1 = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 \\hspace{28mm} if \\quad y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\\\\n",
    "    \\\\\n",
    "    J_2 = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 + 1 - y^{(i)}(w^T x^{(i)} + b) \\qquad otherwise\n",
    "    \\end{cases} </div></div></font>\n",
    " \n",
    "<font face=\"vazir\" size=4>\n",
    "<div dir=rtl> <br>\n",
    "    با مشتق گرفتن در حالت اول خواهیم داشت:\n",
    "<br>\n",
    "<div align=center>\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial J_1}{\\partial w} = w\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial J_1}{\\partial b} = 0\n",
    "    \\end{cases} </div></div></font>\n",
    "    \n",
    "<font face=\"vazir\" size=4>   \n",
    "<div dir=rtl> <br>\n",
    "    در حالت دوم نیز خواهیم داشت:\n",
    "<br>\n",
    "<div align=center>\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial J_2}{\\partial w} = w - C\\sum_{i=1}^{N} y^{(i)}x^{(i)}\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial J_2}{\\partial b} = -C\\sum_{i=1}^{N} y^{(i)}\n",
    "\\end{cases} </div></div></font>\n",
    "    \n",
    "<div dir=rtl><font face=\"vazir\" size=4> <br>\n",
    "(در صورتی که مشتق گرفتن نسبت به بردار در بالا برای شما مشخص نیست، پیشنهاد می‌شود نسبت به تک‌تک درایه‌های بردار $w$ مشتق گرفته و آن‌ها را در یک بردار قرار دهید و به نتایج بالا برسید؛\n",
    "    زیرا همانطور که می‌دانید مشتق گرفتن تابع نسبت به بردار به معنای مشتق گرفتن نسبت به درایه‌های آن بردار می‌باشد!)\n",
    "<br> <br>  برای مشاهده پیاده‌سازی الگوریتم بالا به <a href=\"link to practical section\">نوتبوک عملی</a> مراجعه کنید.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20471565-92ff-46f5-9a11-e47c31a00896",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"svm\">\n",
    "    <h1>\n",
    "        <font family=\"vazir\" color=#0000CD size=7>طبقه‌بند بردار پشتیبان (Support Vector Classifier)</font> \n",
    "    </h1>\n",
    "    <hr>\n",
    "    <font face=\"vazir\" size=4>\n",
    "    همانطور که می‌توانید حدس بزنید و در کلاس نیز اشاره شده است، طبقه‌بند بیشینه‌نمای حاشیه (Maximal Margin Classifier) فقط در شرایطی عملکرد صحیحی دارد که\n",
    "    داده‌های دو کلاس به طور خطی تفکیک‌پذیر باشند. همچنین مرز این مدل توسط نمونه‌هایی مشخص می‌شود که بر روی حاشیه قرار میگیرند؛ در واقع در صورتی که فقط یکی از نمونه‌های آموزشی نزدیک به مرز مقداری تغییر اندازه داشته باشد، مرز تصمیم‌گیری به طور کامل تغییر میکند! \n",
    "    ذکر این نکته حائز اهمیت است که داده‌های ما در واقعیت در اکثر موارد مقداری Noise دارند و بنابراین مشکل بالا می‌تواند اثر نامطلوبی روی مرز تصمیم‌گیری ما داشته باشد. بنابراین این مدل به شدت قابلیت بیش‌برازش شدن (Overfitting) دارند.\n",
    "<br>\n",
    "<br> در ادامه، برای حالت گفته‌شده، مدل Maximal Margin Classifier را با استفاده از Scikit-learn امتحان میکنیم:\n",
    "    </font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c17fe7-2243-4d21-b220-e68b1a04b697",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"limit1\">\n",
    "    <h2>\n",
    "        <font family=\"vazir\" color=#FF8C00 size=6>محدودیت اول: وابستگی به نقاط مرزی  </font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "    <font face=\"vazir\" size=4> \n",
    "خط بهینه در این حالت به نقاط مرزی وابستگی زیادی دارد؛ بنابرین تغییر کوچکی در هر یک از آن‌ها(همچون وجود نویز یا دلایل دیگر) معادله خط جدا کننده را تحت تاثیر شدید قرار می‌دهد؛ بنابراین، این مدل به شدت قابلیت بیش‌برازش دارد.\n",
    "<br>در ادامه بر روی دیتاست اولیه که قابلیت جدایی‌پذیری خطی را دارد، مدل خود را برازش می‌کنیم؛ سپس یک نمونه‌ی نویزی به هر کلاس اضافه میکنیم و اثر آن‌ها را بر روی مرز تصمیم‌گیری مشاهده می‌کنیم:\n",
    "</font>\n",
    "\n",
    "<!-- must add results' images -->\n",
    "\n",
    "<font face=\"vazir\" size=4>\n",
    "همانطور که مشاهده می‌کنیم، در نمودار سمت چپ مرز تصمیم‌گیری رسم شده حاشیه‌ی آن مقدار قابل توجهی دارد؛ درحالیکه در نمودار سمت راست، به دلیل وجود دو نمونه نویزی، مرز تصمیم‌گیری و حاشیه‌های آن تغییراتی داشته‌اند و مقدار حاشیه آن کاهش زیادی داشته است. بنابراین اطمینان ما از مرز تصمیم‌گیری به دلیل کاهش حاشیه‌ی آن، کم خواهد شد.\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe82b1-9bde-41a3-b9f9-e1a6b198b51f",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"solution1\">\n",
    "    <h3>\n",
    "        <font family=\"vazir\" color=#9932CC size=6>راه‌حل: استفاده از Soft Margin Classifier</font> \n",
    "    </h3>\n",
    "    <hr>\n",
    "</div>\n",
    "<font face=\"family\" size=4><div dir=rtl>\n",
    "    برای حل این مشکل می‎توان به‌جای پیدا کردن ابرصفحه‌ای که تمام داده‌های دو کلاس را به طور کامل از یکدیگر جدا نماید، ابرصفحه‌ای را استفاده نمود که <b>بیشتر</b> داده‌های دو کلاس را از یکدیگر تفکیک نماید؛ انگیزه ما از این کار می‌تواند بدین صورت باشد که طبقه‌بند ما حساسیت کمتری به هر نمونه داشته باشد و در نتیجه احتمال کمتری بابت بیش‌برازش شدن داشته باشد. درواقع مدل ما تعداد نسبتا کمی از نمونه‌های آموزشی را اشتباه طبقه‌بندی می‌کند به منظور اینکه بتواند عمومیت\n",
    "    (generalization) بهتری بر داده‌های تست داشته باشد. \n",
    "<br> برای این منظور می‌توان به جای پیدا کردن ابرصفحه‌ای که به دنبال بیشترین حاشیه جهت تفکیک داده‌های دو کلاس است، اجازه دهیم برخی نمونه‌ها حاشیه تعریف شده را نقض کنند؛ البته این تخطی از مرز نیز باید به صورت کنترل‌شده‌ای باشد که باعث عملکرد ضعیف مدل ما نیز نباشد. این کار با اضافه‌شدن متغیرهای جدیدی به مسئله بهینه‌سازی انجام می‌شود که به این متغیرها <b>slack variables</b> می‌گویند و با $\\xi$ نمایش می‌دهند.\n",
    "این متغیر برای هر نمونه مقدار تخطی آن از حاشیه تعریف شده را مشخص می‌کند که در شکل زیر این موضوع به خوبی قابل مشاهده است.\n",
    "</font></div>\n",
    "    \n",
    "<div align=center> <br>\n",
    "<img src=\"resources/SVM-with-soft-margin-kernel-with-different-cases-of-slack-variables.jpg\" width=\"500\">\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl> \n",
    "در واقع در این حالت اجازه می‌دهیم بعضی نمونه‌های آموزشی در سمت اشتباه حاشیه و یا حتی در سمت اشتباه ابرصفحه جداکننده قرار گیرند که این موضوع زمانی که ابرصفحه جداکننده‌ای وجود نداشته باشد غیرقابل اجتناب می‌باشد؛ بنابراین به جای حالت قبل، مرزهای ما حالت نرم‌تری به خود می‌گیرند که به این طبقه‌بند <b>Soft Margin Classifier</b> می‌گویند.\n",
    "    مسئله بهینه‎سازی ما در این حالت به صورت زیر خواهد بود:\n",
    "<div align=center>\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b,\\xi} \\quad & \\frac{1}{2}||w||_2 ^2+C\\sum_{i=1}^{N}{\\xi_{i}}\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1-\\xi_{i}\\\\\n",
    "        &\\xi_i\\geq0 \\quad \\forall i \\in \\{1,2,...,N\\}   \\\\\n",
    "    \\end{aligned}\n",
    "    \\end{equation*} </div>\n",
    "</div>\n",
    "    \n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "<br> در مسئله بهینه‌سازی بالا، $\\xi_1, \\xi_2 , ..., \\xi_N$ همان slack variables می‌باشند و به هر نمونه اجازه می‌دهند که از حاشیه یا ابرصفحه جداکننده تخطی کند؛ هرچند مجموع این خطاها نیز به تابع هدف جهت کمینه‌شدن اضافه شده است و سعی می‌کنیم با این کار، از زیاد شدن این خطا جلوگیری کنیم. این متغیرهای نامنفی در واقع نشان می‌دهند که هر نمونه نسبت به حاشیه و ابرصفحه کجا قرار گرفته است. در صورتی‌که $\\xi_i = 0$ باشد، نمونه متناظر در سمت درستی نسبت به حاشیه قرار گرفته است. در صورتی‌که $\\xi_i > 0$ باشد اما، نمونه $i$م حاشیه را نقض کرده و در صورتی که $\\xi_i > 1$ باشد نیز، نمونه متناظر در سمت اشتباهی نسبت به ابرصفحه جداکننده قرار می‌گیرد. <br>\n",
    "<br> هایپرپارامتر C نوعی regularization parameter است که مشخص می‌کند در تابع هدف مسئله بهینه‌سازی، چقدر به اندازه حاشیه یا به دقت روی داده‌های آموزشی توجه کنیم. \n",
    "    به بیان دیگر، هرچه C بزرگتر باشد، اهمیت عدم اشتباه پیش‌بینی نکردن نمونه‌ها بیشتر می‌شود و در مسئله بهینه‌سازی سعی می‌شود مجموع slack variables کوچک شود؛ حتی اگر حاشیه ما کم شود. در صورتی‌که C کوچک باشد، اهمیت بیشتری به بیشینه کردن حاشیه داده خواهد شد و هزینه کمتری بابت قرارگرفتن نمونه‌ها در سمت اشتباه حاشیه یا ابرصفحه جداکننده به تابع هدف اضافه می‌شود. پس به نوعی، هایپرپارامتر C از طریق کنترل‌کردن عرض حاشیه، trade-off بین بایاس و واریانس مدل را در اختیار دارد و مقدار بهینه آن را باید با استفاده از cross-validation مشخص کنیم. در ادامه سعی می‌کنیم، اثر این هایپرپارامتر را بر روی دیتایی که تفکیک‌پذیر خطی نیست را مشاهده کنیم.\n",
    "</div></font>\n",
    "\n",
    "<!-- must add results' images -->\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "همانطور که از توزیع داده‌ها مشخص هست، داده‌ها در این حالت کاملا تفکیک‌پذیر خطی نیستند؛ در ادامه سعی کنید با کامل‌کردن قطعه کد پایین، اثر هایپرپارامتر C را ترسیم کنید:\n",
    "</div></font>\n",
    "<!-- must add results' images -->\n",
    "\n",
    "<font face=\"vazir\" size=4><div dir=rtl>\n",
    "        همانطور که می‌بینید، با افزایش C حاشیه بدست آمده کوچک‌تر شده و همچنین تعداد نمونه‌های support نیز کمتر شده‌اند؛ یعنی نمونه‌های کمتری در مشخص‌کردن این مرز نقش داشته‌اند. پس می‌توانیم با بدست آوردن مقدار مناسبی برای این هایپرپارامتر، از soft margin classifier جهت طبقه‌بندی استفاده کنیم تا مدل نسبت به نمونه‌های آموزشی robustness بیشتری داشته باشد.\n",
    "<br><br> با وجود تمام خاصیت‌های خوبی که soft margin classifier به مدل اولیه ما اضافه می‌کند، ما همچنان محدود به مرزهای خطی هستیم. پس باید به‌دنبال راهی برای رفع این محدودیت نیز باشیم: <br>\n",
    "</div></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037a06b-971f-4ad7-88a3-c8b11f39387a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
