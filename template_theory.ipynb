{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5ef68b",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=5><div dir=rtl align=center>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "به نام خدا\n",
    "</font>\n",
    "<br> <br>\n",
    "<font size=3>\n",
    "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "<br> <br>\n",
    "<font color=blue size=5>\n",
    "مقدمه‌ای بر یادگیری ماشین\n",
    "</font>\n",
    "\n",
    "<hr/> <br>\n",
    "<font color=red size=6>\n",
    "فصل سوم: یادگیری، ارزیابی و تنظیم‎کردن مدل‎ها \n",
    "<br>\n",
    "</font>\n",
    "<br>\n",
    "نویسندگان: <br> \n",
    "<br>علیرضا گرگوری مطلق، پیمان ناصری، علیرضا حیدری\n",
    "<hr>\n",
    "</div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2644e",
   "metadata": {},
   "source": [
    "<div dir=rtl align=right>\n",
    "<div id=\"table-of-contents\">\n",
    "  <h1>فصل سوم: یادگیري، ارزیابی و تنظیم کردن مدلها</h1>\n",
    "    \n",
    "  <ul>\n",
    "    <li><a href=\"#section-intro\">مقدمه</a>\n",
    "    <li><a href=\"#section-1\">بخش اول</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#1.1 hyperplane\"> 1.1 ابر صفحه</a></li>\n",
    "        <li><a href=\"#1.2 Margin\"> 2.1 حاشیه</a></li>\n",
    "        <li><a href=\"#Maximal Margin Classifier\"> 3.1 طبقه‌بندی بیشینه‌نمای حاشیه</a></li>\n",
    "  </ul>\n",
    "        <li><a href=\"#section-2\">بخش دوم</a>\n",
    "   <ul>\n",
    "        <li><a href=\"#Gradient Descent\">1.2 نزول گرادیان</a></li>\n",
    "        <li><a href=\"#Gradient Descent for Maximal Margin Classifier\"> 2.2 نزول گرادیان برای طبقه بندی بیشینه نمای حاشیه </a></li>\n",
    "  </ul>\n",
    "    <li><a href=\"#section-3\">بخش سوم</a></li>\n",
    "   <ul>\n",
    "       <li><a href=\"#Introducing the Scikit-Learn library\">Scikit-Learn  3.1 معرفی کتابخانه </a></li>\n",
    "   </ul>  \n",
    "       <li><a href=\"#section-4\">بخش چهارم</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#SupportVectorClassifier\">1.4 طبقه‌بندی بردار پشتیبانی </a></li>\n",
    "        <li><a href=\"#svc_first_limit\">2.4محدودیت اول: وابستگی به نقاط مرزی </a></li>\n",
    "        <li><a href=\"#solution1: Soft Margin Classifier\">  3.4راه حل اول: طبقه بندی نرم حاشیه</a></li>\n",
    "        <li><a href=\"#The second limitation: lack of linear separability of the data\">4.4محدودیت دوم: عدم تفکیک‌پذیری خطی داده‌ها</a>           </li>\n",
    "        <li><a href=\"#Second solution: Using Kernel\">5.4راه حل دوم: استفاده از کرنل </a></li>\n",
    "  </ul>\n",
    "        <li><a href=\"#section-5\">بخش پنجم</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#Polynomial Kernel\"> 1.5 کرنل چندجمله‌ای </a></li>\n",
    "        <li><a href=\"#Radial Basis Function\"> 2.5 کرنل RBF</a></li>\n",
    "  </ul>\n",
    "          <li><a href=\"#section-6\">بخش ششم</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#Polynomial Kernel\"> 1.6 چندجمله‌ای </a></li>\n",
    "        <li><a href=\"#Radial Basis Function\"> 2.6 کرنل RBF</a></li>\n",
    "  </ul>\n",
    "        <li><a href=\"#section-7\">بخش هفتم</a>\n",
    "      <ul>\n",
    "         <li><a href=\"#svm decomposition\">1.7 پیچیدگی طبقه بندی بردار پشتیبان</a></li>\n",
    "  </ul>\n",
    "        <li><a href=\"#section-8\">بخش هشتم</a>\n",
    "      <ul>\n",
    "         <li><a href=\"#Multi-Class SVC\"> 1.8 طبقه‌بند بردار پشتیبان چندکلاسه</a></li>\n",
    "         <li><a href=\"#One-vs-Rest (One-vs-All)\">One-vs-Rest (One-vs-All) 2.8</a></li>\n",
    "         <li><a href=\"#One-vs-One\">One-vs-One (One-vs-All) 3.8</a></li>\n",
    "  </ul>\n",
    "            <li><a href=\"#section-9\">بخش نهم</a>\n",
    "      <ul>\n",
    "         <li><a href=\"#Support Vector Regression\"> 1.9 رگرسیون بردار پشتیبان</a></li>\n",
    "   </ul> \n",
    "             <li><a href=\"#section-10\">بخش دهم</a>\n",
    "      <ul>\n",
    "         <li><a href=\"#Hyperparameters setting and model selection\"> 1.10 تنظیم هایپرپارامتر‌ها و انتخاب مدل</a></li>\n",
    "         <li><a href=\"#Evaluation criteria of models\"> 2.10 معیار های مختلف ارزیابی مدل ها در مسائل دسته‌بندی و رگرسیون</a></li>\n",
    "         <li><a href=\"#Calculate accuracy\"> 3.10 محاسبه دقت</a></li>\n",
    "         <li><a href=\"#Confusion Matrix\"> 4.10 ماتریس سردرگمی</a></li>\n",
    "         <li><a href=\"#Precision/Recall tradeoff\"> 5.10 معاوضه دقت و یادآوری</a></li>\n",
    "         <li><a href=\"#ROC Curve\"> 6.10 منحنی ROC</a></li>\n",
    "         <li><a href=\"#AUC\"> 7.10 مساحت زیر منحنی AUC</a></li>\n",
    "           \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee51e14-31fa-4b7d-9cd4-7fb83c5029ac",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"section-intro\">\n",
    "    <h2>\n",
    "        <font color=\"red\" size=5>مقدمه</font>\n",
    "    </h2>\n",
    "    <hr>\n",
    "    <div dir=rtl>\n",
    "        <font face=\"XB Zar\" size=4>\n",
    "            در این فصل قصد داریم که به معرفی مفاهیم پایه‌ای و البته مهمی در یادگیری ماشین بپردازیم. تمرکز ما برای معرفی مفاهیم مرتبط،\n",
    "            مدل <b>ماشین‌های بردار پشتیبان</b>\n",
    "            (Support Vector Machines)\n",
    "می باشد؛ در ابتدا به پیاده‌سازی مدل ساده‌شده‌ای از SVM پرداخته و سپس به معرفی کتابخانه\n",
    "Scikit-Learn\n",
    "که یکی از مهمترین و قدرتمندترین کتابخانه‌های موجود در زمینه یادگیری ماشین است میپردازیم.\n",
    "در ادامه با استفاده از ماژول‌های این کتابخانه مدل‌های پیچیده‌تری را پیاده‌سازی خواهیم کرد و نحوه ارزیابی و انتخاب مدل‌های مناسب را بر اساس معیارهای مدنظرمان فرا خواهیم گرفت.\n",
    "<br> <br>\n",
    "پیش از شروع بحث بهتر است به معرفی ابرصفحه (Hyperplane) و نیز حاشیه (Margin) بپردازیم.\n",
    "<br><b> (در سراسر این فصل فرض می‌شود که $N$ تعداد نمونه‌های ما و $p$ تعداد ویژگی‌های هر نمونه می‌باشد.) \n",
    "        </font>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b095e-2a69-456d-beb9-87ffa36caeec",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"section-4\">\n",
    "    <h1>\n",
    "        <font color=\"red\" size=5>بخش چهارم</font> \n",
    "    </h1>\n",
    "    <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20471565-92ff-46f5-9a11-e47c31a00896",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"SupportVectorClassifier\">\n",
    "    <h2>\n",
    "        <font color=\"orange\" size=5>طبقه‌بند بردار پشتیبان (Support Vector Classifier)</font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "    همانطور که می‌توانید حدس بزنید و در کلاس نیز اشاره شده است، طبقه‌بند بیشینه‌نمای حاشیه (Maximal Margin Classifier) فقط در شرایطی عملکرد صحیحی دارد که\n",
    "    داده‌های دو کلاس به طور خطی تفکیک‌پذیر باشند. همچنین مرز این مدل توسط نمونه‌هایی مشخص می‌شود که بر روی حاشیه قرار میگیرند؛ در واقع در صورتی که فقط یکی از نمونه‌های آموزشی نزدیک به مرز مقداری تغییر اندازه داشته باشد، مرز تصمیم‌گیری به طور کامل تغییر میکند! \n",
    "    ذکر این نکته حائز اهمیت است که داده‌های ما در واقعیت در اکثر موارد مقداری Noise دارند و بنابراین مشکل بالا می‌تواند اثر نامطلوبی روی مرز تصمیم‌گیری ما داشته باشد. بنابراین این مدل به شدت قابلیت بیش‌برازش شدن (Overfitting) دارند.\n",
    "<br>\n",
    "<br> در ادامه، برای حالت گفته‌شده، مدل Maximal Margin Classifier را با استفاده از Scikit-learn امتحان میکنیم:\n",
    "    </font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c17fe7-2243-4d21-b220-e68b1a04b697",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"svc_first_limit\">\n",
    "    <h2>\n",
    "        <font color=\"orange\" size=5>محدودیت اول: وابستگی به نقاط مرزی  </font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "    <font face=\"XB Zar\" size=4> \n",
    "خط بهینه در این حالت به نقاط مرزی وابستگی زیادی دارد؛ بنابرین تغییر کوچکی در هر یک از آن‌ها(همچون وجود نویز یا دلایل دیگر) معادله خط جدا کننده را تحت تاثیر شدید قرار می‌دهد؛ بنابراین، این مدل به شدت قابلیت بیش‌برازش دارد.\n",
    "<br>در ادامه بر روی دیتاست اولیه که قابلیت جدایی‌پذیری خطی را دارد، مدل خود را برازش می‌کنیم؛ سپس یک نمونه‌ی نویزی به هر کلاس اضافه میکنیم و اثر آن‌ها را بر روی مرز تصمیم‌گیری مشاهده می‌کنیم:\n",
    "</font>\n",
    "    \n",
    "    images\n",
    "\n",
    "<font face=\"XB Zar\" size=4>\n",
    "همانطور که مشاهده می‌کنیم، در نمودار سمت چپ مرز تصمیم‌گیری رسم شده حاشیه‌ی آن مقدار قابل توجهی دارد؛ درحالیکه در نمودار سمت راست، به دلیل وجود دو نمونه نویزی، مرز تصمیم‌گیری و حاشیه‌های آن تغییراتی داشته‌اند و مقدار حاشیه آن کاهش زیادی داشته است. بنابراین اطمینان ما از مرز تصمیم‌گیری به دلیل کاهش حاشیه‌ی آن، کم خواهد شد.\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe82b1-9bde-41a3-b9f9-e1a6b198b51f",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "    <h2>\n",
    "        <font color=\"orange\" size=5>راه‌حل: استفاده از Soft Margin Classifier</font> \n",
    "    </h2>\n",
    "    <hr>\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "        <font>\n",
    "    برای حل این مشکل می‎توان به‌جای پیدا کردن ابرصفحه‌ای که تمام داده‌های دو کلاس را به طور کامل از یکدیگر جدا نماید، ابرصفحه‌ای را استفاده نمود که **بیشتر** داده‌های دو کلاس را از یکدیگر تفکیک نماید؛ انگیزه ما از این کار می‌تواند بدین صورت باشد که طبقه‌بند ما حساسیت کمتری به هر نمونه داشته باشد و در نتیجه احتمال کمتری بابت بیش‌برازش شدن داشته باشد. درواقع مدل ما تعداد نسبتا کمی از نمونه‌های آموزشی را اشتباه طبقه‌بندی می‌کند به منظور اینکه بتواند عمومیت\n",
    "    (generalization) بهتری بر داده‌های تست داشته باشد. \n",
    "<br> برای این منظور می‌توان به جای پیدا کردن ابرصفحه‌ای که به دنبال بیشترین حاشیه جهت تفکیک داده‌های دو کلاس است، اجازه دهیم برخی نمونه‌ها حاشیه تعریف شده را نقض کنند؛ البته این تخطی از مرز نیز باید به صورت کنترل‌شده‌ای باشد که باعث عملکرد ضعیف مدل ما نیز نباشد. این کار با اضافه‌شدن متغیرهای جدیدی به مسئله بهینه‌سازی انجام می‌شود که به این متغیرها **slack variables** می‌گویند و با $\\xi$ نمایش می‌دهند.\n",
    "این متغیر برای هر نمونه مقدار تخطی آن از حاشیه تعریف شده را مشخص می‌کند که در شکل زیر این موضوع به خوبی قابل مشاهده است.\n",
    "</div>\n",
    "    \n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/SVM-with-soft-margin-kernel-with-different-cases-of-slack-variables.jpg\" width=\"500\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "در واقع در این حالت اجازه می‌دهیم بعضی نمونه‌های آموزشی در سمت اشتباه حاشیه و یا حتی در سمت اشتباه ابرصفحه جداکننده قرار گیرند که این موضوع زمانی که ابرصفحه جداکننده‌ای وجود نداشته باشد غیرقابل اجتناب می‌باشد؛ بنابراین به جای حالت قبل، مرزهای ما حالت نرم‌تری به خود می‌گیرند که به این طبقه‌بند **Soft Margin Classifier** می‌گویند.\n",
    "    مسئله بهینه‎سازی ما در این حالت به صورت زیر خواهد بود:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b,\\xi} \\quad & \\frac{1}{2}||w||_2 ^2+C\\sum_{i=1}^{N}{\\xi_{i}}\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1-\\xi_{i}\\\\\n",
    "        &\\xi_i\\geq0 \\quad \\forall i \\in \\{1,2,...,N\\}   \\\\\n",
    "    \\end{aligned}\n",
    "    \\end{equation*} </div>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br> در مسئله بهینه‌سازی بالا، $\\xi_1, \\xi_2 , ..., \\xi_N$ همان slack variables می‌باشند و به هر نمونه اجازه می‌دهند که از حاشیه یا ابرصفحه جداکننده تخطی کند؛ هرچند مجموع این خطاها نیز به تابع هدف جهت کمینه‌شدن اضافه شده است و سعی می‌کنیم با این کار، از زیاد شدن این خطا جلوگیری کنیم. این متغیرهای نامنفی در واقع نشان می‌دهند که هر نمونه نسبت به حاشیه و ابرصفحه کجا قرار گرفته است. در صورتی‌که $\\xi_i = 0$ باشد، نمونه متناظر در سمت درستی نسبت به حاشیه قرار گرفته است. در صورتی‌که $\\xi_i > 0$ باشد اما، نمونه $i$م حاشیه را نقض کرده و در صورتی که $\\xi_i > 1$ باشد نیز، نمونه متناظر در سمت اشتباهی نسبت به ابرصفحه جداکننده قرار می‌گیرد. <br>\n",
    "<br> هایپرپارامتر C نوعی regularization parameter است که مشخص می‌کند در تابع هدف مسئله بهینه‌سازی، چقدر به اندازه حاشیه یا به دقت روی داده‌های آموزشی توجه کنیم. \n",
    "    به بیان دیگر، هرچه C بزرگتر باشد، اهمیت عدم اشتباه پیش‌بینی نکردن نمونه‌ها بیشتر می‌شود و در مسئله بهینه‌سازی سعی می‌شود مجموع slack variables کوچک شود؛ حتی اگر حاشیه ما کم شود. در صورتی‌که C کوچک باشد، اهمیت بیشتری به بیشینه کردن حاشیه داده خواهد شد و هزینه کمتری بابت قرارگرفتن نمونه‌ها در سمت اشتباه حاشیه یا ابرصفحه جداکننده به تابع هدف اضافه می‌شود. پس به نوعی، هایپرپارامتر C از طریق کنترل‌کردن عرض حاشیه، trade-off بین بایاس و واریانس مدل را در اختیار دارد و مقدار بهینه آن را باید با استفاده از cross-validation مشخص کنیم. در ادامه سعی می‌کنیم، اثر این هایپرپارامتر را بر روی دیتایی که تفکیک‌پذیر خطی نیست را مشاهده کنیم.\n",
    "</div></font>\n",
    "    <div>\n",
    "        images\n",
    "    </div>\n",
    "    <font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که از توزیع داده‌ها مشخص هست، داده‌ها در این حالت کاملا تفکیک‌پذیر خطی نیستند؛ در ادامه سعی کنید با کامل‌کردن قطعه کد پایین، اثر هایپرپارامتر C را ترسیم کنید:\n",
    "        <div>\n",
    "            images\n",
    "        </div>\n",
    "\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "        همانطور که می‌بینید، با افزایش C حاشیه بدست آمده کوچک‌تر شده و همچنین تعداد نمونه‌های support نیز کمتر شده‌اند؛ یعنی نمونه‌های کمتری در مشخص‌کردن این مرز نقش داشته‌اند. پس می‌توانیم با بدست آوردن مقدار مناسبی برای این هایپرپارامتر، از soft margin classifier جهت طبقه‌بندی استفاده کنیم تا مدل نسبت به نمونه‌های آموزشی robustness بیشتری داشته باشد.\n",
    "<br><br> با وجود تمام خاصیت‌های خوبی که soft margin classifier به مدل اولیه ما اضافه می‌کند، ما همچنان محدود به مرزهای خطی هستیم. پس باید به‌دنبال راهی برای رفع این محدودیت نیز باشیم: <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4321d47-e59d-41f0-b2ba-c1e016132218",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
